{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "from tqdm.auto import tqdm\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\n",
    "def IoU(boxA, boxB):\n",
    "  # determine the (x, y)-coordinates of the intersection rectangle\n",
    "  xA = max(boxA[0], boxB[0])\n",
    "  yA = max(boxA[1], boxB[1])\n",
    "  xB = min(boxA[2], boxB[2])\n",
    "  yB = min(boxA[3], boxB[3])\n",
    "  # compute the area of intersection rectangle\n",
    "  interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "  # compute the area of both the prediction and ground-truth rectangles\n",
    "  boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "  boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "  # compute the intersection over union by taking the intersection area\n",
    "  # and dividing it by the sum of prediction + ground-truth areas (interesection area)\n",
    "  IoU = interArea / float(boxAArea + boxBArea - interArea)\n",
    "  return IoU\n",
    "\n",
    "# load any word embeddings with format used by Word2Vec, glove, etc.\n",
    "def load_word_embeddings(file):\n",
    "  embeddings_dict = {}\n",
    "  head = True\n",
    "  with open(file, 'r') as f:\n",
    "    for line in f:\n",
    "      if head:\n",
    "        head = False\n",
    "        continue\n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      vector = np.asarray(values[1:], \"float32\")\n",
    "      embeddings_dict[word] = vector\n",
    "  return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_score(query, image):\n",
    "  # find spatial score using IoU\n",
    "  # box coordinates are normalized for 64x64 image\n",
    "  # TODO: fix box coord bug\n",
    "  q_box_s, q_box_o = query['data']['s_box'], query['data']['o_box']\n",
    "  i_box_s, i_box_o = image ['data']['s_box'], image['data']['o_box']\n",
    "  d = np.sqrt( np.sum( (np.array(q_box_s) - np.array(i_box_s))**2 + (np.array(q_box_o) - np.array(i_box_o))**2 ) )\n",
    "  s_spatial = math.exp(-d)\n",
    "  #s_spatial = (IoU(q_box_s, i_box_s) + IoU(q_box_o, i_box_o))/2  + 0.00001\n",
    "  \n",
    "  # predicate may have preposition such as \"parked on\"\n",
    "  val1 = query['data']['predicate'].split()\n",
    "  p_q_embed = embeds[val1[0]]\n",
    "  val2 = image['data']['predicate'].split()\n",
    "  p_i_embed = embeds[val2[0]]  \n",
    "  #pred_score = np.dot(p_q_embed, p_i_embed)\n",
    "    \n",
    "  # try dot product between concatenated query vector\n",
    "  q_vec = np.concatenate(( embeds[ query['data']['subject'] ], p_q_embed, embeds[ query['data']['object'] ]), axis=None)/np.sqrt(3)\n",
    "  i_vec = np.concatenate(( embeds[ image['data']['subject'] ], p_i_embed, embeds[ image['data']['object'] ]), axis=None)/np.sqrt(3)                                                                                                      \n",
    "  ###s_semantic = np.dot(q_vec, i_vec) + 0.00001\n",
    "  \n",
    "  #print( query['data']['subject'], image['data']['subject'], query['data']['object'], image['data']['object']) \n",
    "  if query['data']['subject'] == image['data']['subject'] and query['data']['object'] == image['data']['object'] and  \\\n",
    "    query['data']['predicate'] == image['data']['predicate']:\n",
    "    s_semantic = 1.0\n",
    "  elif query['data']['subject'] == image['data']['subject'] and query['data']['object'] == image['data']['object']:\n",
    "    s_semantic = 0.66\n",
    "  elif query['data']['subject'] == image['data']['subject']:\n",
    "    s_semantic = 0.33\n",
    "  elif query['data']['object'] == image['data']['object']:\n",
    "    s_semantic = 0.33\n",
    "  #elif query['data']['predicate'] == image['data']['predicate']:\n",
    "   # s_semantic = 0.33\n",
    "  else:\n",
    "    s_semantic = 0\n",
    "  \n",
    "  #s_semantic = np.dot(q_vec, i_vec)\n",
    "    \n",
    "  return s_spatial*s_semantic\n",
    "  #return (s_spatial+s_semantic)/2\n",
    "\n",
    "  #alpha = 0.7\n",
    "  #return alpha*s_spatial + (1-alpha)*s_semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_score(query, image):\n",
    "  # find spatial score using IoU\n",
    "  # box coordinates are normalized for 64x64 image\n",
    "  # TODO: fix box coord bug\n",
    "  q_box_s, q_box_o = query['data']['s_box'], query['data']['o_box']\n",
    "  i_box_s, i_box_o = image ['data']['s_box'], image['data']['o_box']\n",
    "  d = np.sqrt( np.sum( (np.array(q_box_s) - np.array(i_box_s))**2 + (np.array(q_box_o) - np.array(i_box_o))**2 ) )\n",
    "  s_spatial = math.exp(-d)\n",
    "  #s_spatial =  (IoU(q_box_s, i_box_s) + IoU(q_box_o, i_box_o))/2 \n",
    "  return s_spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_score(query, image):\n",
    "  \n",
    "  # predicate may have preposition such as \"parked on\"\n",
    "  val1 = query['data']['predicate'].split()\n",
    "  p_q_embed = embeds[val1[0]]\n",
    "  val2 = image['data']['predicate'].split()\n",
    "  p_i_embed = embeds[val2[0]]  \n",
    "\n",
    "  # try dot product between concatenated query vector\n",
    "  q_vec = np.concatenate(( embeds[ query['data']['subject'] ], p_q_embed, embeds[ query['data']['object'] ]), axis=None)/np.sqrt(3)\n",
    "  i_vec = np.concatenate(( embeds[ image['data']['subject'] ], p_i_embed, embeds[ image['data']['object'] ]), axis=None)/np.sqrt(3)                                                                                                      \n",
    "  s_semantic = np.dot(q_vec, i_vec)\n",
    "\n",
    "  return s_semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# load queries\n",
    "with open('queries.json') as f:\n",
    "  queries = json.load(f)\n",
    "\n",
    "# load images\n",
    "with open('docs.json') as f:\n",
    "  images = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# load word embeddings\n",
    "embedding_file = \"./word_embeddings/numberbatch-en-19.08.txt\"\n",
    "embeds = load_word_embeddings(embedding_file)\n",
    "print('Done loading word embedding! whee!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# relevance threshold\n",
    "relevance_thresh = 0.25\n",
    "# Q-D matrix of relevances\n",
    "relevances = np.zeros((len(queries),len(images))) \n",
    "all_scores = np.zeros((len(queries),len(images)))\n",
    "all_labels =  []\n",
    "\n",
    "# iterate over all queries\n",
    "print('Processing', len(queries), 'queries')\n",
    "\n",
    "for n, query in enumerate(tqdm(queries, leave=False)):\n",
    "  #print('n =', n)\n",
    "  scores = []\n",
    "  labels = []\n",
    "  #print('query:', query['data']['subject'],query['data']['predicate'],query['data']['object'])\n",
    "  \n",
    "  # iterate over image db\n",
    "  for m, image in enumerate(images):\n",
    "    #print(image)\n",
    "    # sort by this\n",
    "    score = relevance_score(query, image)\n",
    "    #print('image:', image['data']['subject'],image['data']['predicate'],image['data']['object'])\n",
    "    #print('relevance score = ', score)\n",
    "    scores.append(score)\n",
    "    labels.append(image['data']['subject'] + ' ' + image['data']['predicate'] + ' ' + image['data']['object'])\n",
    "  \n",
    "  \n",
    "  # display retrieval\n",
    "#   scores = np.array(scores)\n",
    "#   labels = np.array(labels)\n",
    "#   n_label = len(scores)\n",
    "#   index = scores.argsort(axis=0)[::-1][:n_label]\n",
    "#   labels = labels[index]\n",
    "#   #print('retrieved images:')\n",
    "#   #print(labels[0:10])\n",
    "#   # calculate recall (these are \"ideal\" relevances for this example)\n",
    "#   relevances[n][np.where(scores[index] > relevance_thresh)] = 1\n",
    "  all_scores[n] = scores\n",
    "  all_labels.append(labels)\n",
    "\n",
    "# \"ground truth\"\n",
    "relevances = all_scores > relevance_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_scores_sp = np.zeros((len(queries),len(images)))\n",
    "for n, query in enumerate(tqdm(queries, leave=False)):\n",
    "  # iterate over image db\n",
    "  for m, image in enumerate(images):\n",
    "     sys_scores_sp[n,m] = spatial_score(query, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_scores_sem = np.zeros((len(queries),len(images)))\n",
    "for n, query in enumerate(tqdm(queries, leave=False)):\n",
    "  # iterate over image db\n",
    "  for m, image in enumerate(images):\n",
    "     sys_scores_sem[n,m] = semantic_score(query, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_scores.shape\n",
    "plt.matshow(all_scores)\n",
    "plt.colorbar()\n",
    "plt.matshow(relevances)\n",
    "plt.matshow(sys_scores_sp)\n",
    "plt.title('spatial')\n",
    "plt.matshow(sys_scores_sem)\n",
    "plt.title('semantic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n, bins, patches = plt.hist(x=all_scores.flatten(), bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85, label='spatio-semantic')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('relevance')\n",
    "plt.ylabel('count')\n",
    "plt.title('distribution of relevance scores (spatio-semantic)')\n",
    "\n",
    "n, bins, patches = plt.hist(x=sys_scores_sp.flatten(), bins='auto', color='red',\n",
    "                            alpha=0.7, rwidth=0.85, label='spatial')\n",
    "n, bins, patches = plt.hist(x=sys_scores_sem.flatten(), bins='auto', color='green',\n",
    "                            alpha=0.7, rwidth=0.85, label='semantic')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys_scores_sp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-372adc9fc64c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# BUG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# other system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msys_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys_scores_sp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msys_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msys_scores_sort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys_scores_sp' is not defined"
     ]
    }
   ],
   "source": [
    "# BUG\n",
    "# other system\n",
    "sys_scores = sys_scores_sp\n",
    "sys_index = np.argsort(sys_scores, axis=1)[:,::-1]\n",
    "sys_scores_sort = np.take_along_axis(sys_scores, sys_index, 1)\n",
    "sys_relevances_sort = np.take_along_axis(relevances, sys_index, 1)\n",
    "sys_recalls = np.cumsum(sys_relevances_sort, 1)/(np.sum(sys_relevances_sort, 1)[:,None] + 1e-12) # [:,None] transposes matrix\n",
    "sys_mean_recall = np.mean(sys_recalls, axis=0) # column-wise\n",
    "\n",
    "# ideal: no index needed as we are sorting relevances itself\n",
    "relevances_sort = np.sort(relevances, axis=1)[:,::-1]\n",
    "recalls = np.cumsum(relevances_sort, 1)/(np.sum(relevances_sort, 1)[:,None] + 1e-12) # [:,None] transposes matrix\n",
    "mean_recall = np.mean(recalls, axis=0) # column-wise\n",
    "plt.matshow(relevances_sort)\n",
    "\n",
    "32\n",
    "fig = plt.figure()\n",
    "plt.grid()\n",
    "plt.xlim(0, len(mean_recall))\n",
    "plt.xlabel('k')\n",
    "plt.ylim(0,1.01)\n",
    "plt.ylabel('Recall at k')\n",
    "x = np.arange(1,len(mean_recall)+1)\n",
    "#pdb.set_trace()\n",
    "plt.plot(x, mean_recall, label=\"ideal\")\n",
    "plt.plot(x, sys_mean_recall, label=\"sys\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic\n",
    "sys_scores = sys_scores_sem\n",
    "sys_index = np.argsort(sys_scores, axis=1)[:,::-1]\n",
    "sys_relevances_sort = np.take_along_axis(relevances, sys_index, 1)\n",
    "sys_recalls = np.cumsum(sys_relevances_sort, 1)/(np.sum(sys_relevances_sort, 1)[:,None] + 1e-12) # [:,None] transposes matrix\n",
    "sys_mean_recall_sem = np.mean(sys_recalls, axis=0) # column-wise\n",
    "\n",
    "# spatial\n",
    "sys_scores = sys_scores_sp\n",
    "sys_index = np.argsort(sys_scores, axis=1)[:,::-1]\n",
    "sys_relevances_sort = np.take_along_axis(relevances, sys_index, 1)\n",
    "sys_recalls = np.cumsum(sys_relevances_sort, 1)/(np.sum(sys_relevances_sort, 1)[:,None] + 1e-12) # [:,None] transposes matrix\n",
    "sys_mean_recall_sp = np.mean(sys_recalls, axis=0) # column-wise\n",
    "\n",
    "# ideal: no index needed as we are sorting relevances itself\n",
    "relevances_sort = np.sort(relevances, axis=1)[:,::-1]\n",
    "recalls = np.cumsum(relevances_sort, 1)/(np.sum(relevances_sort, 1)[:,None] + 1e-12) # [:,None] transposes matrix\n",
    "mean_recall = np.mean(recalls, axis=0) # column-wise\n",
    "plt.matshow(relevances_sort)\n",
    "\n",
    "# best-case or ideal recall\n",
    "#pdb.set_trace()\n",
    "# plot best-case recall\n",
    "fig = plt.figure()\n",
    "plt.grid()\n",
    "plt.xlim(0, len(mean_recall))\n",
    "plt.xlabel('k')\n",
    "plt.ylim(0,1.01)\n",
    "plt.ylabel('Recall at k')\n",
    "x = np.arange(1,len(mean_recall)+1)\n",
    "#pdb.set_trace()\n",
    "plt.plot(x, mean_recall, label=\"ideal (spatio-semantic)\")\n",
    "plt.plot(x, sys_mean_recall_sp, label=\"spatial\")\n",
    "plt.plot(x, sys_mean_recall_sem, label=\"semantic\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevances, all_scores, all_labels\n",
    "sys_scores = sys_scores_sp\n",
    "sys_index = np.argsort(sys_scores, axis=1)[:,::-1]\n",
    "sys_scores_sort = np.take_along_axis(sys_scores, sys_index, 1)\n",
    "sys_all_scores_sort = np.take_along_axis(all_scores, sys_index, 1)\n",
    "sys_relevances_sort = np.take_along_axis(relevances, sys_index, 1)\n",
    "\n",
    "#plt.matshow(sys_relevances_sort)\n",
    "#plt.show()\n",
    "#breakpoint()\n",
    "\n",
    "sys_labels_sort = []\n",
    "for n, labels in enumerate(all_labels): \n",
    "  order = sys_index[n]\n",
    "  labels = [labels[i] for i in order]\n",
    "  sys_labels_sort.append(labels)\n",
    "\n",
    "for n, query in enumerate(queries):  \n",
    "  # display retrieval\n",
    "  print('query:', query['data']['subject'],query['data']['predicate'],query['data']['object'])\n",
    "  print('retrieved images:')\n",
    "  pprint.pprint(sys_labels_sort[n][0:10])\n",
    "  pprint.pprint(sys_relevances_sort[n][0:10])\n",
    "  pprint.pprint(sys_all_scores_sort[n][0:10])\n",
    "  breakpoint()\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "define expts by task: each expt based upon different tasks\n",
    "different rel thresh\n",
    "blend of spatial vs semaTic "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
