{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\n",
    "def IoU(boxA, boxB):\n",
    "  # determine the (x, y)-coordinates of the intersection rectangle\n",
    "  xA = max(boxA[0], boxB[0])\n",
    "  yA = max(boxA[1], boxB[1])\n",
    "  xB = min(boxA[2], boxB[2])\n",
    "  yB = min(boxA[3], boxB[3])\n",
    "  # compute the area of intersection rectangle\n",
    "  interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "  # compute the area of both the prediction and ground-truth rectangles\n",
    "  boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "  boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "  # compute the intersection over union by taking the intersection area\n",
    "  # and dividing it by the sum of prediction + ground-truth areas (interesection area)\n",
    "  IoU = interArea / float(boxAArea + boxBArea - interArea)\n",
    "  return IoU\n",
    "\n",
    "def box_distance(boxA, boxB):\n",
    "  # determine the (x, y)-coordinates of the intersection rectangle\n",
    "  xA = boxA[0] - boxB[0]\n",
    "  yA = boxA[1] - boxB[1]\n",
    "  xB = boxA[2] - boxB[2]\n",
    "  yB = boxA[3] - boxB[3]\n",
    "  # compute the area of intersection rectangle\n",
    "  interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "  # compute the area of both the prediction and ground-truth rectangles\n",
    "  boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "  boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "  # compute the intersection over union by taking the intersection area\n",
    "  # and dividing it by the sum of prediction + ground-truth areas (interesection area)\n",
    "  IoU = interArea / float(boxAArea + boxBArea - interArea)\n",
    "  return IoU\n",
    "\n",
    "# load any word embeddings with format used by Word2Vec, glove, etc.\n",
    "def load_word_embeddings(file):\n",
    "  embeddings_dict = {}\n",
    "  head = True\n",
    "  with open(file, 'r') as f:\n",
    "    for line in f:\n",
    "      if head:\n",
    "        head = False\n",
    "        continue\n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      vector = np.asarray(values[1:], \"float32\")\n",
    "      embeddings_dict[word] = vector\n",
    "  return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_score(query, image):\n",
    "  # find spatial score using IoU\n",
    "  # box coordinates are normalized for 64x64 image\n",
    "  # TODO: fix box coord bug\n",
    "  q_box_s, q_box_o = query['data']['s_box'], query['data']['o_box']\n",
    "  i_box_s, i_box_o = image ['data']['s_box'], image['data']['o_box']\n",
    "  s_spatial = (IoU(q_box_s, q_box_o) + IoU(i_box_s, i_box_o))/2\n",
    "  #s_spatial = 0\n",
    "\n",
    "  \n",
    "#   # use word embedding to determine semantic similarity between query terms\n",
    "#   obj_score = np.dot(embeds[ query['data']['object'] ], embeds[ image['data']['object'] ] )\n",
    "  \n",
    "  # predicate may have preposition such as \"parked on\"\n",
    "  val1 = query['data']['predicate'].split()\n",
    "  p_q_embed = embeds[val1[0]]\n",
    "  val2 = image['data']['predicate'].split()\n",
    "  p_i_embed = embeds[val2[0]]  \n",
    "  #pred_score = np.dot(p_q_embed, p_i_embed)\n",
    "    \n",
    "#   #print('predicate score:', val1[0], val2[0], '->', pred_score) \n",
    "#   #pred_score = np.dot(embeds[ query['data']['predicate'] ], embeds[ image['data']['predicate'] ] ) \n",
    "#   subj_score = np.dot(embeds[ query['data']['subject'] ], embeds[ image['data']['subject'] ] )\n",
    "#   #s_semantic = (obj_score + pred_score + subj_score)/3\n",
    "\n",
    "  # try dot product between concatenated query vector\n",
    "  q_vec = np.concatenate(( embeds[ query['data']['subject'] ], p_q_embed, embeds[ query['data']['object'] ]), axis=None)/np.sqrt(3)\n",
    "  i_vec = np.concatenate(( embeds[ image['data']['subject'] ], p_i_embed, embeds[ image['data']['object'] ]), axis=None)/np.sqrt(3)                                                                                                      \n",
    "  s_semantic = np.dot(q_vec, i_vec)\n",
    "  \n",
    "  alpha = 0.7\n",
    "\n",
    "  return s_spatial*s_semantic\n",
    "  #return alpha*s_spatial*(1-alpha)*s_semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_score(query, image):\n",
    "  # find spatial score using IoU\n",
    "  # box coordinates are normalized for 64x64 image\n",
    "  # TODO: fix box coord bug\n",
    "  q_box_s, q_box_o = query['data']['s_box'], query['data']['o_box']\n",
    "  i_box_s, i_box_o = image ['data']['s_box'], image['data']['o_box']\n",
    "  s_spatial = (IoU(q_box_s, q_box_o) + IoU(i_box_s, i_box_o))/2\n",
    "  return s_spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_score(query, image):\n",
    "  \n",
    "  # predicate may have preposition such as \"parked on\"\n",
    "  val1 = query['data']['predicate'].split()\n",
    "  p_q_embed = embeds[val1[0]]\n",
    "  val2 = image['data']['predicate'].split()\n",
    "  p_i_embed = embeds[val2[0]]  \n",
    "  pred_score = np.dot(p_q_embed, p_i_embed)\n",
    "\n",
    "  # try dot product between concatenated query vector\n",
    "  q_vec = np.concatenate(( embeds[ query['data']['subject'] ], p_q_embed, embeds[ query['data']['object'] ]), axis=None)/np.sqrt(3)\n",
    "  i_vec = np.concatenate(( embeds[ image['data']['subject'] ], p_i_embed, embeds[ image['data']['object'] ]), axis=None)/np.sqrt(3)                                                                                                      \n",
    "  s_semantic = np.dot(q_vec, i_vec)\n",
    "\n",
    "  return s_semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.3 ms, sys: 3.87 ms, total: 36.1 ms\n",
      "Wall time: 36.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load queries\n",
    "with open('queries.json') as f:\n",
    "  queries = json.load(f)\n",
    "\n",
    "# load images\n",
    "with open('docs.json') as f:\n",
    "  images = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading word embedding! whee!\n",
      "CPU times: user 29.3 s, sys: 1.21 s, total: 30.5 s\n",
      "Wall time: 30.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load word embeddings\n",
    "embedding_file = \"./word_embeddings/numberbatch-en-19.08.txt\"\n",
    "embeds = load_word_embeddings(embedding_file)\n",
    "print('Done loading word embedding! whee!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 queries\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353ae4d57ec549df986d67dac89f9928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-10-2099e1a14382>(31)relevance_score()\n",
      "-> i_vec = np.concatenate(( embeds[ image['data']['subject'] ], p_i_embed, embeds[ image['data']['object'] ]), axis=None) # /np.sqrt(3)\n",
      "(Pdb) n\n",
      "> <ipython-input-10-2099e1a14382>(32)relevance_score()\n",
      "-> s_semantic = np.dot(q_vec, i_vec)\n",
      "(Pdb) print(d)\n",
      "1.0000045\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# relevance threshold\n",
    "relevance_thresh = 0.25\n",
    "# Q-D matrix of relevances->recalls\n",
    "relevances = np.zeros((len(queries),len(images))) \n",
    "all_scores = np.zeros((len(queries),len(images)))\n",
    "\n",
    "# iterate over all queries\n",
    "print('Processing', len(queries), 'queries')\n",
    "\n",
    "for n, query in enumerate(tqdm(queries, leave=False)):\n",
    " # print('n =', n)\n",
    "  scores = []\n",
    "  labels = []\n",
    "  #print('query:', query['data']['subject'],query['data']['predicate'],query['data']['object'])\n",
    "  \n",
    "  # iterate over image db\n",
    "  for m, image in enumerate(images):\n",
    "    #print(image)\n",
    "    # sort by this\n",
    "    score = relevance_score(query, image)\n",
    "    #print('image:', image['data']['subject'],image['data']['predicate'],image['data']['object'])\n",
    "    #print('relevance score = ', score)\n",
    "    scores.append(score)\n",
    "    labels.append([image['data']['subject'],image['data']['predicate'],image['data']['object']])\n",
    "  \n",
    "  \n",
    "  # display retrieval\n",
    "#   scores = np.array(scores)\n",
    "#   labels = np.array(labels)\n",
    "#   n_label = len(scores)\n",
    "#   index = scores.argsort(axis=0)[::-1][:n_label]\n",
    "#   labels = labels[index]\n",
    "#   #print('retrieved images:')\n",
    "#   #print(labels[0:10])\n",
    "#   # calculate recall (these are \"ideal\" relevances for this example)\n",
    "#   relevances[n][np.where(scores[index] > relevance_thresh)] = 1\n",
    "  all_scores[n] = scores\n",
    "\n",
    "relevances = all_scores > relevance_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, query in enumerate(tqdm(queries, leave=False)):\n",
    "  # iterate over image db\n",
    "  for m, image in enumerate(images):\n",
    "     sys_scores[n,m] = spatial_score(query, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, query in enumerate(tqdm(queries, leave=False)):\n",
    "  # iterate over image db\n",
    "  for m, image in enumerate(images):\n",
    "     sys_scores[n,m] = semantic_score(query, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores.shape\n",
    "plt.matshow(all_scores)\n",
    "#plt.colorbar()\n",
    "plt.matshow(relevances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_scores = all_scores.copy()\n",
    "sys_scores = sys_scores + 0.1*np.random.random(sys_scores.shape)\n",
    "sys_index = np.argsort(sys_scores, axis=1)[:,::-1]\n",
    "\n",
    "relevances_sort = np.take_along_axis(relevances, sys_index, 1)\n",
    "recalls = np.cumsum(relevances_sort, 1)/(np.sum(relevances_sort, 1)[:,None] + 1e-12) # [:,None] transposes matrix\n",
    "mean_recall = np.mean(recalls, axis=0) # column-wise\n",
    "\n",
    "# best-case or ideal recall\n",
    "#pdb.set_trace()\n",
    "# plot best-case recall\n",
    "fig = plt.figure()\n",
    "plt.grid()\n",
    "plt.xlim(0, len(mean_recall))\n",
    "plt.xlabel('k')\n",
    "plt.ylim(0,1.01)\n",
    "plt.ylabel('Recall at k')\n",
    "x = np.arange(1,len(mean_recall)+1)\n",
    "#pdb.set_trace()\n",
    "plt.plot(x, mean_recall)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys_scores = all_scores.copy()\n",
    "sys_scores = sys_scores + 0.1*np.random.random(sys_scores.shape)\n",
    "sys_index = np.argsort(sys_scores, axis=1)[:,::-1]\n",
    "\n",
    "relevances_sort = np.take_along_axis(relevances, sys_index, 1)\n",
    "recalls = np.cumsum(relevances_sort, 1)/(np.sum(relevances_sort, 1)[:,None] + 1e-12) # [:,None] transposes matrix\n",
    "mean_recall = np.mean(recalls, axis=0) # column-wise\n",
    "\n",
    "# best-case or ideal recall\n",
    "#pdb.set_trace()\n",
    "# plot best-case recall\n",
    "fig = plt.figure()\n",
    "plt.grid()\n",
    "plt.xlim(0, len(mean_recall))\n",
    "plt.xlabel('k')\n",
    "plt.ylim(0,1.01)\n",
    "plt.ylabel('Recall at k')\n",
    "x = np.arange(1,len(mean_recall)+1)\n",
    "#pdb.set_trace()\n",
    "plt.plot(x, mean_recall)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake\n",
    "sys_scores = all_scores.copy()\n",
    "sys_scores = sys_scores + 0.1*np.random.random(sys_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other system\n",
    "sys_index = np.argsort(sys_scores, axis=1)[:,::-1]\n",
    "sys_relevances_sort = np.take_along_axis(relevances, sys_index, 1)\n",
    "sys_recalls = np.cumsum(sys_relevances_sort, 1)/(np.sum(sys_relevances_sort, 1)[:,None] + 1e-12) # [:,None] transposes matrix\n",
    "sys_mean_recall = np.mean(sys_recalls, axis=0) # column-wise\n",
    "\n",
    "# ideal\n",
    "relevances_sort = np.sort(relevances, axis=1)[:,::-1]\n",
    "recalls = np.cumsum(relevances_sort, 1)/(np.sum(relevances_sort, 1)[:,None] + 1e-12) # [:,None] transposes matrix\n",
    "mean_recall = np.mean(recalls, axis=0) # column-wise\n",
    "\n",
    "# best-case or ideal recall\n",
    "#pdb.set_trace()\n",
    "# plot best-case recall\n",
    "fig = plt.figure()\n",
    "plt.grid()\n",
    "plt.xlim(0, len(mean_recall))\n",
    "plt.xlabel('k')\n",
    "plt.ylim(0,1.01)\n",
    "plt.ylabel('Recall at k')\n",
    "x = np.arange(1,len(mean_recall)+1)\n",
    "#pdb.set_trace()\n",
    "plt.plot(x, mean_recall, label=\"ideal\")\n",
    "plt.plot(x, sys_mean_recall, label=\"sys\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define expts by task: each expt based upon different tasks\n",
    "different rel thresh\n",
    "blend of spatial vs semaTic "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
